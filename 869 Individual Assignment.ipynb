{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a6c672",
   "metadata": {},
   "source": [
    "# Question 1 (60%):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a9530",
   "metadata": {},
   "source": [
    "### Submission:\n",
    "Submit the Python code that completes the tasks below. Your code for each model should output the confusion matrix and the cost matrix with a clear explanation of how it was calculated. Additionally, provide an explanation of how you chose the optimal threshold in each scenario/model, preferably with a plot of loss or profit versus threshold.\n",
    "The could should run and produce the same results you obtained. I suggest to reset your kernel before running any standalone cell to avoid an involuntary carry over of variables (it is a good practice).\n",
    "\n",
    "Tips are given as lines of code to guide you, but feel free to use a different approach.\n",
    "\n",
    "\n",
    "### Grading Criteria:\n",
    "\n",
    "#### **Data Preparation: (20 points)**\n",
    "Loading and Encoding, split the dataset into features and target variable, and into training and testing sets, scaling.\n",
    "#### **Model Training and Initial Evaluation: (20 points)**\n",
    "Run the logistic model. Calculate the confusion matrix, accuracy, and classification report for the default threshold.\n",
    "#### **Adjusting the Classification Threshold for both scenarios (contingency reserve and overall profit): (30 points)**\n",
    "New cost and Value Matrix Calculation. Explanation of changes (if any) made to the thershold.\n",
    "#### **Advanced Evaluation and Visualization: (30 points)**\n",
    "Repeat with at least 3 ML classification algorithms. Explain if any model beat the initial logistic regression model. Estimate the marginal benefit contribution of the new (if any) chosen model.\n",
    "\n",
    "### Context:\n",
    "A financial institution wants to use a logistic regression model to calculate the contingency reserve for unpaid credits. For this exercise, you will assume that loans classified as \"Good\" will be paid back, while loans classified as \"Bad\" will, at some point, stop being paid. On average, 20% of the initial loan value is lost for \"Bad\" loans. Create a value matrix that assigns a financial value to each of the four quadrants of the confusion matrix generated by the logistic regression model.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "1. **Load and Preprocess the Data:**\n",
    "    - Load the dataset from 'GermanCredit.csv'.\n",
    "    - Encode the target variable 'Class'.\n",
    "    - Split the dataset into features and the target variable.\n",
    "    - Split the data into training and testing sets.\n",
    "    - Standardize Separately: Fit the scaler only on the training data and use it to transform both the training and testing data to avoid data leakage.\n",
    "\n",
    "\n",
    "2. **Train the Logistic Regression Model:**\n",
    "    - Train the logistic regression model on the scaled training data and predict probabilities on the scaled test data. Later you will modify the threshold.\n",
    "\n",
    "\n",
    "3. **Adjust the Classification Threshold:**\n",
    "    - Define a classification threshold (e.g., 0.3) or leave the default value to predict the target variable based on the predicted probabilities.\n",
    "\n",
    "\n",
    "4. **Evaluate the Model:**\n",
    "    - Calculate the confusion matrix for the predictions.\n",
    "    - Calculate the accuracy and classification report for the model.\n",
    "    \n",
    "5. **Create the Value Matrix:**\n",
    "    - Assume the average loan amount is \\$1,000.\n",
    "    - Assign a value of 20\\% of the loan amount ($200) as the financial loss for \"Bad\" loans that are incorrectly classified as \"Good\" (False Positives), that is loans that are granted to customers that defaulted.\n",
    "    - No loss for True Positives (loans correctly classified as \"Good\").\n",
    "    - No loss for True Negatives (loans correctly classified as \"Bad\").\n",
    "    - For False Negatives (good loans incorrectly classified as \"Bad\"), assume the financial institution does not need to reserve for the contingency of the loan not being paid, as these customers are not given the loan.\n",
    "\n",
    "6. **Calculate the Cost Matrix:**\n",
    "    - Multiply the confusion matrix by the value matrix to calculate the cost matrix.\n",
    "\n",
    "7. **Visualize the Results:**\n",
    "    - Plot the confusion matrix.\n",
    "    - Plot the cost matrix side by side with the confusion matrix.\n",
    "    \n",
    "More tasks follow below.\n",
    "\n",
    "I added a few lines, in case you need some orientation. This is my style, which may not be yours or the TA's, but if you need the names of functions to search to get you started, leverage them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3c58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "# file_path = 'GermanCredit.csv'\n",
    "# data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode the target variable 'Class'\n",
    "\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature variables using training data statistics\n",
    "\n",
    "\n",
    "# Train a logistic regression model\n",
    "\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "\n",
    "\n",
    "# Define the threshold or leave the default one.\n",
    "# threshold = 0.3\n",
    "\n",
    "# Predict the target variable based on the threshold. The line will change if you use the default value.\n",
    "# y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# classification_report_str = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "\n",
    "\n",
    "# sklearn.metrics.confusion_matrix function reports the confusion matrix in the following format:\n",
    "# [[TN, FP],\n",
    "# [FN, TP]]\n",
    "# you can put the values associated with the confusion matrix following the same lay out\n",
    "# value_matrix = np.array([\n",
    "#     [TN_Loss, FP_loss],  \n",
    "#     [FN_Loss, TP_Loss]         \n",
    "# ])\n",
    "\n",
    "# and then to calculate the cost matrix, you can do an element wise multiplication\n",
    "# cost_matrix = conf_matrix * value_matrix\n",
    "# you can sum all four quadrants for the cost_matrix to give the overal loss or profit: np.sum(cost_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3681e33",
   "metadata": {},
   "source": [
    "8. **Calculating Contingency Reserve:**\n",
    "    - Define a function to calculate the total financial impact for a given threshold.\n",
    "    - Loop through a range of thresholds and compute the financial impact for each.\n",
    "\n",
    "9. **Finding the Optimal Threshold:**\n",
    "    - Identify the threshold that minimizes the total financial impact defined as the amount of the contingency reserve.\n",
    "\n",
    "10. **Plotting the Results:**\n",
    "    - Plot the financial impact versus the thresholds to visualize the optimal threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4959b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would suggest to create a function to calculate total financial impact at each threshold and then use this\n",
    "# function in a loop to evaluate the loss at each threshold\n",
    "\n",
    "# you can add all the elements of a value matrix in one line\n",
    "# total_impact = np.sum(cost_matrix)\n",
    "# if you use a function, return total_impact or similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881cd51",
   "metadata": {},
   "source": [
    "11. **Print the classification report and the confusion matrix** \n",
    "    - For the optimal threshold: Explain what do you see in terms of the elements of the confusion matrix and model performance metrics to minimize the overall contigency reserve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e14709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d6635f",
   "metadata": {},
   "source": [
    "#### Your anwer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734f4c5",
   "metadata": {},
   "source": [
    "The product manager brings to the attention of the treasury that the overall profit is not maximized by this approach as it is short sighted. An opportunity cost is being left behind when you deny a credit to a good customer, and also giving a loan to a good customer also gives a profit, both factors are not considered in the overall initial calculation. Assume the profit is 5% of the loan, this will go towards a loan granted to a good customer and the loss opportunity of a loan denied to a good customer.\n",
    "\n",
    "Task:\n",
    "    \n",
    "12. Recalculate the cost matrix defining the vaule of the four quadrants (a loan not granted to a customer who is going to default stays at \\$0, to simplify things we are not considering loss avoidance) according to the new considerations and, using the same functions created above, recalculate the optimal threshold to maximize profit. Explain how the model is adjusted to this new loss structure. Would you change your threshold to maximize the overall profit? Is there a range of thresholds where the finanancial institution makes a profit and a range where it losses money?\n",
    "\n",
    "hint: before you were minimizing a contingency reserve, now you are maximizing profit, you can think of the contingency reserve as a loss. We are going to ignore for the purpose of this exercise the time value of when the loss is realized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af37d326",
   "metadata": {},
   "source": [
    "One possible interpretation to the new cost matrix, signs can be the opposite, as far as it is consistent.\n",
    "\n",
    "True Positives (TP): Positive, as they represent realized profit of correctly given loans.\n",
    "\n",
    "True Negatives (TN): Zero or neutral, as there is no financial impact. Loan not given to a bad customer.\n",
    "\n",
    "False Positives (FP): Negative, as they represent a financial loss. Loan given to a bad customer.\n",
    "\n",
    "False Negatives (FN): Negative, as they represent an opportunity cost or missed profit. Loan not given to a \n",
    "good customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a725d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here. You are re-running the model with a new value matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27156a4",
   "metadata": {},
   "source": [
    "Management have heard that Machine Learning techiques can produce better results than the old logistic regression.\n",
    "\n",
    "13. Try a few ML algorithms (at least 3 algorithms like Decision Tree, Random Forest, SVM, NN, remember to use Grid or Random search and CV) and see if you can beat the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81694fa0",
   "metadata": {},
   "source": [
    "Random forest with no CV and tuning did not improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "531162b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62696bf9",
   "metadata": {},
   "source": [
    "Random Forest with CV and grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f4b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# # Define the Random Forest model\n",
    "# rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Define the hyperparameters grid to search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [xx, xx, xx],\n",
    "#     'max_depth': [None, xx, xx, xx],\n",
    "#     'min_samples_split': [xx, xx, xx],\n",
    "#     'min_samples_leaf': [xx, xx, xx]\n",
    "# }\n",
    "\n",
    "# # Use GridSearchCV to find the best hyperparameters\n",
    "# grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "# grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Get the best estimator\n",
    "# best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# # Predict probabilities on the test set\n",
    "# y_prob = best_rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc582af0",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6aa00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86ee964",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a64a047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# # Define the MLP model\n",
    "# mlp_model = MLPClassifier(random_state=42, max_iter=2000)\n",
    "\n",
    "# # Define the hyperparameters grid to search\n",
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(xx, xx), (xx, xx), (xx, xx)],\n",
    "#     'activation': ['tanh', 'relu'],\n",
    "#     'solver': ['adam', 'sgd'],\n",
    "#     'alpha': [xx, xx, xx],\n",
    "#     'learning_rate': ['constant', 'adaptive']\n",
    "# }\n",
    "\n",
    "# # Use GridSearchCV to find the best hyperparameters\n",
    "# grid_search = GridSearchCV(estimator=mlp_model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "# grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Get the best estimator\n",
    "# best_mlp_model = grid_search.best_estimator_\n",
    "\n",
    "# # Predict probabilities on the test set\n",
    "# y_prob = best_mlp_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d995a",
   "metadata": {},
   "source": [
    "14. Write conclusions. What is the marginal increase of your best ML model? Are all models equally robust, that is, some may hae a narrow range of thresholds where they yield a profit, others a wide range? Which one would you chose? Is it worth?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee3b5c5",
   "metadata": {},
   "source": [
    "#### your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028bc4a",
   "metadata": {},
   "source": [
    "# Question 2 (40%):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc567588",
   "metadata": {},
   "source": [
    "# Assignment: Customer Segmentation Using Unsupervised Learning\n",
    "\n",
    "## Objective:\n",
    "A common task in marketing is to classify customers based on their spending habits. This is an unsupervised exercise, as customers don't have a label on their foreheads :) with a classification for each company they interact with. You will be provided with a synthetic dataset, `diverse_synthetic_customer_segmentation_dataset.csv`, containing various features related to customer demographics, behavior, and preferences. The data is well-structured and designed to form distinguishable clusters. Segment customers into personas using K-means clustering to identify distinct customer segments.\n",
    "\n",
    "The aim is to determine the optimal number of segments in which to classify customers and provide an interpretation of the clusters \\ segments created. To that purpose, you will create a K-means model, perform clustering, and provide an interpretation of the identified customer personas. The data has more than two dimensions, so it cannot be visualized easily. In this exercise, we will create an approximated visualization using principal components. The first two principal components can be used to visualize the clusters.\n",
    "\n",
    "\n",
    "## Features in the Dataset:\n",
    "1. **CustomerID**: Unique identifier for each customer.\n",
    "2. **Age**: Age of the customer.\n",
    "3. **Gender**: Gender of the customer (0: Male, 1: Female, 2: Other).\n",
    "4. **Annual Income (k$)**: Annual income of the customer in thousands.\n",
    "5. **Spending Score (1-100)**: A score assigned based on customer behavior and spending nature.\n",
    "6. **Tenure**: Number of years the customer has been with the company.\n",
    "7. **Purchase Frequency**: Number of purchases made by the customer in the last year.\n",
    "8. **Average Transaction Value**: Average value of transactions made by the customer.\n",
    "9. **Product Category Preference**: Preferred category of products (0: Electronics, 1: Clothing, 2: Groceries).\n",
    "10. **Online Activity**: Number of online interactions (e.g., website visits, social media interactions).\n",
    "\n",
    "## Tasks:\n",
    "1. **Data Preprocessing**:\n",
    "   - Load the dataset and inspect its structure.\n",
    "   - Handle missing values and any infinite values if any.\n",
    "   - Convert categorical features to numerical values if necessary (encode).\n",
    "   - Standardize the features to ensure each feature contributes equally to the clustering.\n",
    "\n",
    "2. **K-means Clustering**:\n",
    "   - Apply the K-means clustering algorithm to segment the customers into an initial set of clusters.\n",
    "\n",
    "3. **Determine the Optimal Number of Clusters**:\n",
    "   - Use the Elbow Method to identify the optimal number of clusters.\n",
    "   - Calculate and plot the Silhouette Score for different numbers of clusters.\n",
    "   - Decide on the optimal number of clusters based on the evaluation metrics.\n",
    "   - Re-run the K-means clustering with the optimal number of clusters.\n",
    "   \n",
    "4. **PCA and Clustering Visualization**:\n",
    "   - Perform Principal Component Analysis (PCA) to reduce the dimensionality of the dataset.\n",
    "   - Visualize the clusters in the reduced PCA space using the first two principal components to visually evaluate the clustering quality.\n",
    "\n",
    "5. **Interpretation of Clusters**:\n",
    "   - Analyze the cluster centers to understand the characteristics of each customer segment.\n",
    "   - Assign cluster labels to the customers in the dataset.\n",
    "   - Provide a detailed interpretation of the identified personas, describing key features that distinguish each cluster.\n",
    "\n",
    "6. **Report**:\n",
    "   - Prepare a report summarizing your findings. Include the following sections:\n",
    "     - **Data Preprocessing**: Explain the steps taken to clean and preprocess the data.\n",
    "     - **PCA Visualization**: Show the PCA 2D plot and discuss the clustering results in the reduced space. Did K-means did a good job?\n",
    "     - **Cluster Interpretation**: Provide a detailed interpretation of each identified persona.\n",
    "## Submission:\n",
    "- Submit your code, plots, and report in a Jupyter Notebook printed as a PDF document.\n",
    "- Ensure your code is well-commented and your report is well-structured and clear.\n",
    "\n",
    "## Grading Criteria:\n",
    "- **Data Preprocessing (10%)**: Quality of data cleaning, handling missing values, and feature standardization.\n",
    "- **Optimal Clusters Determination (40%)**: Correct application of the K-means algorithm and accurate clustering results. Appropriate use of evaluation metrics and justification for the chosen number of clusters.\n",
    "- **PCA Visualization (10%)**: Effective use of PCA for visualization and clear presentation of the clusters.\n",
    "- **Cluster Interpretation (40%)**: Comprehensive and insightful interpretation of the identified personas.\n",
    "\n",
    "Good luck, and happy clustering!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0ebdf",
   "metadata": {},
   "source": [
    "I will use PCA to visualize. Clusters on scaled data.\n",
    "kmeans.fit(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47ff7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.cluster import KMeans\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the dataset\n",
    "# file_path = 'diverse_synthetic_customer_segmentation_dataset.csv'\n",
    "# data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# # Apply PCA for visualization purposes\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_data = pca.fit_transform(scaled_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c824f",
   "metadata": {},
   "source": [
    "Your cluster segment / interpretation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746cd5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
